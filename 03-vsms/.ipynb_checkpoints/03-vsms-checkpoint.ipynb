{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ed9f880ca681d98b0c6a6985f56f6fa",
     "grade": false,
     "grade_id": "cell-8570cfd12f5ca3ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 3: Vector Space Models\n",
    "\n",
    "## Released: 04.02.2021 7:00\n",
    "## Deadline: 15.02.2021 at 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e4b56e3b40c75b9656e6429752d6a53",
     "grade": false,
     "grade_id": "cell-d8ae628155888deb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this assignment, we're going to represent documents as points in some high-dimensional vector space. We will try to model their semantics so that similar documents would be close in that space. We're going to work with the song lyrics of three artists. More precisely, our goal is to build a system that can suggest what artist could have written a song when presented with the lyrics of a new, previously unseen, song (with a query). We're going to use several approaches to representing songs (several Vector Space Models):\n",
    "1. songs as sparse vectors of lemma counts\n",
    "2. songs as sparse vectors weighted by tf-idf\n",
    "3. songs as sparse vectors weighted by PPMI\n",
    "4. songs as dense vectors derived from sparse vectors of lemma counts\n",
    "5. songs as dense vectors derived from sparse vectors weighted by tf-idf\n",
    "6. songs as dense vectors derived from sparse vectors weighted by PPMI\n",
    "\n",
    "Hopefully, your models will be able to tell the artists apart! We will evaluate the models by looking at what songs are the closest to a new song in these vector spaces. The best system should find only the songs by the same artist. Don't worry if some of the terms above sound confusing, they are all going to be explained below. \n",
    "\n",
    "Every song will be represented as a vector of values related to word frequencies in this song. The simple intuition behind the method is that word frequencies in a song capture what a corresponding document is about, its meaning. This approach is count-based. Vector spaces are typically constructed from some kind of co-occurrence matrix. We will be using a term-document matrix in this assignment.\n",
    "\n",
    "In general, the creation of count-based vector models consists of the following steps:\n",
    "1. Linguistic processing of text.\n",
    "2. Frequency matrix building.\n",
    "3. Mathematical processing of the matrix elements.\n",
    "4. Dimensionality reduction\n",
    "5. Evaluation\n",
    "\n",
    "We're going to use song lyrics of three artists working in different music genres: **Pulp** (britpop, indie pop), **Princess Nokia** (rap), **At the Drive-In** (post-hardcore, emo).\n",
    "\n",
    "Albums for training:\n",
    "\n",
    "* Pulp: Different Class, His'n'Hers, We Love Life\n",
    "* Princess Nokia: Metallic butterfly, Everything sucks, Everything is beautiful\n",
    "* At the Drive-In: Acrobatic Tenement, Relationship of Command, Vaya\n",
    "Albums for testing:\n",
    "\n",
    "* Pulp: This Is Hardcore\n",
    "* Princess Nokia: 1992\n",
    "* At the Drive-In: In/Casino/Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "620cf0f76dd46de491d62fa6edffa802",
     "grade": false,
     "grade_id": "cell-f3ea63a2d24a1037",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Task 1: Text pre-processing](#task_1)\n",
    "    * [Step 1.1: Tokenization and normalization](#subtask_1_1)\n",
    "* [Task 2: Term-document matrix creation](#task_2)\n",
    "    * [Step 2.1: Create a term-document matrix](#subtask_2_1)\n",
    "* [Task 3: Mathematical processing of matrix elements](#task_3)\n",
    "    * [Step 3.1: tf-idf](#subtask_3_1)\n",
    "    * [Step 3.2: PPMI](#subtask_3_2)\n",
    "* [Task 4: Dense vectors](#task_4)\n",
    "    * [Step 4.1: Truncated SVD (LSI)](#subtask_4_1)\n",
    "* [Task 5: Place queries into the right vector space](#task_5)\n",
    "    * [Step 5.1: Term-document matrix for queries](#subtask_5_1)\n",
    "* [Task 6: Cosine Similarity](#task_6)\n",
    "    * [Step 6.1: Compute cosine similarity](#subtask_6_1)\n",
    "* [Task 7: Quality evaluation](#task_7)    \n",
    "    * [Step 7.1: Find N closest documents to a quesry song](#subtask_7_1)\n",
    "    * [Step 7.2: Compare models](#subtask_7_2)\n",
    "    * [Step 7.3: Choose the best model](#subtask_7_2)\n",
    "* [Checklist before submission](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e8aad2bc682abdde5c715e32276f0aa",
     "grade": false,
     "grade_id": "cell-5044486f6a1cd371",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## Text pre-processing\n",
    "## 1.1 <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### Tokenization and normalization (1 point)\n",
    "\n",
    "As you already know, we always start by preparing text data to fit our needs. The first step of our process will be **tokenization** (as usual). The second step will be **normalization**. Normalization helps reduce superficially different strings to one spelling. For example, strings that differ in their capitalization: \"Run\" and \"run\", or the strings that differ in their grammatical form: \"cat\" and \"cats\". these different strings of characters often convey essentially identical meanings, so it makes sense to unify them into one string in order to get the full distributional statistics. The most common types of normalization for document representations are **lowercasing**, **stemming** and **lemmatization**.\n",
    "\n",
    "**Stemmers** remove morphological affixes from words, leaving only the word stem. For example, words \"jumping\", “jumped”, and “jumper” will be reduced to the stem \"jump\". Stemming is just an approximation of a proper morphological analysis and uses a set of rules of thumb, so for forms like \"fly\", \"flies\", \"flying\" it will assign \"fli\" and it's not a proper English word. Stemming works well for languages like English, but might not be optimal for morphologically-rich languages like Finnish. **Lemmatizers** assign a group of word forms their lemma (dictionary form), so in the case of \"fly\"/\"flies\"/\"flying\" we get \"fly\", they are doing proper morphological analysis.\n",
    "\n",
    "In this assignment, we're going to use [Stanza](https://stanfordnlp.github.io/stanza/index.html) for tokenization and lemmatization.\n",
    "\n",
    "Create a function in a cell below. It should take the name of the text document:\n",
    "1. read it\n",
    "2. tokenize and lemmatize it\n",
    "3. lowercase lemmas\n",
    "4. remove lemmas that are present in a stop word list\n",
    "\n",
    "\n",
    "HINT1: look [here](https://github.com/stanfordnlp/stanza/blob/master/demo/Stanza_Beginners_Guide.ipynb) for Stanza tutorial\n",
    "\n",
    "HINT2: lowercase only after lemmatization, case affects lemmatization result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4df5505e7b4f6588343b245eeca9c3d5",
     "grade": false,
     "grade_id": "cell-028c78a8e5a12c2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install stanza\n",
    "# import stanza\n",
    "# print(\"Downloading English model...\")\n",
    "# stanza.download('en')\n",
    "\n",
    "\n",
    "def tokenize_and_normalize(file_name, stopwords):\n",
    "    \"\"\"Tokenizes, lemmatizes, lowercases and removes stop words.\n",
    "    \n",
    "    this function takes in a path to a song, reads the song file,\n",
    "    tokenizes it into words, then lemmatizes and lowercases these words.\n",
    "    finally, stopwords given to the function are removed from the list of song lemmas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a text file\n",
    "    stopwords : list of strings\n",
    "        stopwords that should be removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_song : list of strings\n",
    "        a song represented as a list of its lemmas\n",
    "    \"\"\"\n",
    "    \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma',  verbose=False)\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        contents=f.read()\n",
    "        f.close()\n",
    "    preprocess=nlp(contents)\n",
    "    normalized_song=[]\n",
    "    for sentence in preprocess.sentences:\n",
    "        for word in sentence.words:\n",
    "            lemma=word.lemma.casefold()\n",
    "            if lemma not in stopwords:\n",
    "                normalized_song.append(lemma)        \n",
    "    return normalized_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b83cc172f0f627a2cebba3cf3f345b84",
     "grade": true,
     "grade_id": "cell-ccd00641165e99e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_song_path = \"/coursedata/03_vsms/dummy_song.txt\"\n",
    "dummy_stop_words = ['lalala', '.']\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(tokenize_and_normalize(dummy_song_path,dummy_stop_words)), list)\n",
    "# check that it's a list of strings\n",
    "assert_equal(type(tokenize_and_normalize(dummy_song_path,dummy_stop_words)[0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_normalized_dummy_song = ['this','be','my','awesome','song',\n",
    "                                 'i','sing','everywhere','i','go',',',\n",
    "                                 'lalada',',','dalala',',','yep','yep','!','ohhhhh','ohhhhhh','yeaaahhhhh',\n",
    "                                 'and','i','have','parted','with','my','sanity','!']\n",
    "assert_equal(tokenize_and_normalize(dummy_song_path,dummy_stop_words), correct_normalized_dummy_song)\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA\n",
    "song1 = '/coursedata/03_vsms/songs_train/at_the_drive_in_vaya/6_300_mhz.txt'\n",
    "assert_equal(len(tokenize_and_normalize(song1, dummy_stop_words)), 174)\n",
    "assert(all([l.lower() for l in tokenize_and_normalize(song1, dummy_stop_words)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cef7ffdb5b397fc342f138a897c0e56",
     "grade": false,
     "grade_id": "cell-897cbf1afc516283",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Now we can prepare our songs. \n",
    "Run the cell below to get a list of pre-processed songs. We're also going to keep track for each artist of what are the indices of songs in this list.\n",
    "\n",
    "The songs will be stored in `normalized_songs_train`. The dictionary with indices from this list will be kept in `normalized_songs_index_train`\n",
    "\n",
    "Note that it's going to take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1610c71b46d5388d4df690b75e91e2fc",
     "grade": false,
     "grade_id": "cell-2fcc51ebdae2230f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "print(\"Downloading stop words...\")\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def add_song(song_dict, artist, index):\n",
    "    \"\"\"Writes a song index into a dictiionary of artists and their song indices\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    song_dict: dict {artist:list of indices}\n",
    "        dictionary of artist names with a list of indices of their songs\n",
    "    artist : str\n",
    "        artist name - a key to which to add value\n",
    "    index : int\n",
    "        value, an index of a song in a list of all songs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    song_dict : dict {atrist:lits of song indices}\n",
    "        song_dict with one new index added\n",
    "    \"\"\"\n",
    "    \n",
    "    if song_dict[artist] == None:\n",
    "        song_dict[artist] = [index]\n",
    "    else:\n",
    "        song_dict[artist].append(index)   \n",
    "    return song_dict\n",
    "\n",
    "\n",
    "# collecting the paths to albums and their songs\n",
    "train_songs = glob.glob('/coursedata/03_vsms/songs_train/*/*')\n",
    "# getting stopwords\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "# lets remember indices for each song, we'll need them later\n",
    "normalized_songs_index_train  = {'pulp':None, 'princess_nokia':None, 'at_the_drive_in':None}\n",
    "normalized_songs_train = []\n",
    "\n",
    "\n",
    "for i, song_path in enumerate(train_songs):\n",
    "    \n",
    "    normalized_songs_train.append(tokenize_and_normalize(song_path, stop_words_english))\n",
    "    \n",
    "    if 'pulp' in song_path:\n",
    "        normalized_songs_index_train = add_song(normalized_songs_index_train,'pulp', i)\n",
    "    \n",
    "    if 'princess_nokia' in song_path:\n",
    "        normalized_songs_index_train = add_song(normalized_songs_index_train,'princess_nokia',i)\n",
    "     \n",
    "    if 'at_the_drive_in' in song_path:\n",
    "        normalized_songs_index_train = add_song(normalized_songs_index_train,'at_the_drive_in',i)\n",
    "            \n",
    "print('There are:', len(normalized_songs_index_train['pulp']), 'songs by Pulp')\n",
    "print('There are:', len(normalized_songs_index_train['princess_nokia']), 'songs by Princess Nokia')\n",
    "print('There are:', len(normalized_songs_index_train['at_the_drive_in']), 'songs by At the Drive-In')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebc3adefe3f483f5556ff4c3d4dc599c",
     "grade": false,
     "grade_id": "cell-f7c984365f51779c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## Term-document matrix creation\n",
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Create a term-document matrix (3 points)\n",
    "Now that step one (linguistic processing) is complete, we can go on to compute statistics from the documents.\n",
    "\n",
    "We will be using **term-document matrix**. Formally, in a term-document matrix $X$, each column $x_i$ corresponds to a document $D_i$, each row $x_j$ corresponds to a term $T_j$, and an element $x_{ij}$ is the frequency of a term $T_j$ in a document $D_i$.\n",
    "\n",
    "Write a function that takes in a list of pre-processed songs and puts their statistics into a term-document matrix. Keep also a list of lemma types corresponding to the rows, we'll need them later.\n",
    "\n",
    "\n",
    "HINT1: you can use collections.Counter to collect statistics for each song.\n",
    "\n",
    "HINT2: you can look at the tests, to examine some useful examples of functions performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb74983f64c139783af1f866a5eb773d",
     "grade": false,
     "grade_id": "cell-50bc668ab6453a56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_term_doc_matrix(songs_normalized):\n",
    "    \"\"\" Constructs a frequency term-document matrix\n",
    "    \n",
    "    this function takes in a list of songs and returns a term-document matrix\n",
    "    the rows are lemma types, the columns are songs \n",
    "    the rows should be sorted alphabetically\n",
    "    the order of the columns should be preserved as it's given in songs_normalized\n",
    "    the cell values are a number of times a lemma was seen in a song\n",
    "    the value should be zero, if a lemma is absent from a song\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    songs_normalized : a list of lists of strings [['a','a','b'], ['a','b','c']]\n",
    "        a list of songs represented as a list of lemmas\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matrix : numpy array\n",
    "        a matrix where columns are songs and rows are lemma types,\n",
    "        the cells of the matrix contain lemma counts in a song,\n",
    "        the lemmas for rows are sorted alphabetically\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        \n",
    "    sorted_vocab : list of strings\n",
    "        a list of all the lemma types used in all songs (the rows of our matrix)\n",
    "        the words should be strings sorted alphabetically\n",
    "        for the example above it should be ['a','b','c']\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return matrix, sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0089d85aca598d5b4cfb6500fcb998a0",
     "grade": true,
     "grade_id": "cell-8c9de22bc0b55f2a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_songs = [['la','la','la','oh',',','woo',\"uuuuuh\"],\n",
    "               ['oh', 'la','la','la',\"tarara\",'tadada', 'blaaa', 'blaaa', '!', '!', '!']]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_term_doc_matrix(dummy_songs)[0].shape, (9, 2))\n",
    "# check that the matrix is a numpy ndarray\n",
    "assert_equal(type(create_term_doc_matrix(dummy_songs)[0]), np.ndarray)\n",
    "# check that the vocabulary is a list\n",
    "assert_equal(type(create_term_doc_matrix(dummy_songs)[1]), list)\n",
    "# check that the vocabulary is a list of strings\n",
    "assert_equal(type(create_term_doc_matrix(dummy_songs)[1][0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the vocabulary is sorted properly\n",
    "assert_equal(create_term_doc_matrix(dummy_songs)[1], ['!', ',', 'blaaa', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo'])\n",
    "# check the count of 'la' in the first song\n",
    "assert_equal(create_term_doc_matrix(dummy_songs)[0][3][0], 3)\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_td_dummy_matrix = np.array([[0., 3.],\n",
    "                                    [1., 0.],\n",
    "                                    [0., 2.],\n",
    "                                    [3., 3.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [1., 0.]])\n",
    "\n",
    "assert_array_equal(create_term_doc_matrix(dummy_songs)[0], correct_td_dummy_matrix)\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_term_doc_matrix(normalized_songs_train)[0].shape, (3249, 99))\n",
    "# check the first and last lemma\n",
    "assert_equal(create_term_doc_matrix(normalized_songs_train)[1][0], '!')\n",
    "assert_equal(create_term_doc_matrix(normalized_songs_train)[1][-1], '’ma')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c00cc3c0d35480967a0fecd2a2ded08",
     "grade": false,
     "grade_id": "cell-36b8993f83640303",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's make a term-frequency matrix for our songs\n",
    "\n",
    "Run the cell below to collect the statistics for the songs in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a14c346407ef402110e159db9458964e",
     "grade": false,
     "grade_id": "cell-df635a8414aeb58a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "td_matrix, sorted_vocabulary = create_term_doc_matrix(normalized_songs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "653588172de8362473e154d9c2ef4ce9",
     "grade": false,
     "grade_id": "cell-2c9e25bb552215f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 3 <a class=\"anchor\" id=\"task_3\"></a>\n",
    "## Mathematical processing of matrix elements\n",
    "\n",
    "Simple raw frequency counts are not the best at the task of measuring the similarity between documents. The information theory states that the more probable the event, the less information it contains. We've already removed the stop words, but we can go even further. For the VSM models, shared frequent words like \"run\" or \"boy\" in two documents are less discriminative for semantics than shared surprising words like \"kerning\" or \"typeface\". Thus, it is only logical to give more weight to rare words and take some weight from frequent words. \n",
    "\n",
    "To downplay the value of common words some weighting schemes can be applied to a matrix. Two popular approaches are **tf-idf** and **PPMI**. They both are based on the information theory idea that surprising events should carry more weight than expected events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8e363c3c29b759f2db4151c78c91f55",
     "grade": false,
     "grade_id": "cell-473e71d95233c90e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.1 <a class=\"anchor\" id=\"subtask_3_1\"></a>\n",
    "### tf-idf (3 points)\n",
    "\n",
    "The **term frequency–inverse document frequency (tf-idf)** algorithm is a weighting scheme for term-document matrices. It captures the intuition that when a word is frequent in a document but is rare in other documents, it carries a lot of information specific for that particular document, thus it should get more weight. In practice, the **tf-idf** is a combination of two statistics: **term frequency** and **inverse document frequency**. \n",
    "\n",
    "\n",
    "**Term frequency** - the number of times a word type $t$ appears in a document $d$\n",
    "\n",
    "$tf_{t,d} = count(t, d)$\n",
    "\n",
    "\n",
    "The second statistic of tf-idf is the **inverse document frequency**. It is based on a notion that a less specific term would be used in more documents in a collection than the more specific ones, so the weight of a less specific term should be downplayed. Formally, **idf** is the total number $N$ of documents in a collection, divided by the number of documents $d_{ft}$ containing the term $t$. The fewer documents contain the term, the larger the **idf** becomes. Due to a large number of documents in many collections, $idf$ is usually squashed with a log function. So the resulting term can be obtained in the following way:\n",
    "\n",
    "**Document frequency** $df_t$ - the number of documents containing $t$\n",
    "\n",
    "**Inverse document frequency** - the total number $N$ of documents divided by the $df_t$\n",
    "\n",
    "$idf_t = \\log_{10}(\\frac{N}{d_{ft}})$\n",
    "\n",
    "Finally, the *tf-idf* score for a word is just a term frequency multiplied by the inverse document frequency:\n",
    "\n",
    "$w_{t,d} = tf_{t,d} × id_{ft}$\n",
    "\n",
    "#### How to weigh a query document with tf-idf?\n",
    "When you have a new document that you want to compare to the documents in the collection, you need to represent it the same way as your old documents. Suppose, your query document already looks like a vector with the dimensions as the vocabulary of your collection. That is, we disregard any new words that the query document might contain.\n",
    "\n",
    "The **tf** of this query document **depends only on the query** itself and is computed the same way as for the training documents. \n",
    "\n",
    "The **idf** is different. It **depends only on the statistics of the documents in our collection**. That means, we don't add the new document to the statistics, but just use the number we had for the existing collection.\n",
    "#### task description\n",
    "Write a function, that takes in a term-document matrix and weights it with the **tf-idf** scheme. To save us time later, our function will both give out a matrix of tf-idf weighted values and a vector of idf values for words in the collection. We will use this vector when representing new documents in the same vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94ddcaf3b7d0e5d069674424b4a14ba8",
     "grade": false,
     "grade_id": "cell-33fcd5d339672048",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tf_idf(td_matrix):\n",
    "    \"\"\" Weighs a term-document matrix of raw counts with tf-idf scheme\n",
    "    \n",
    "    this function takes in a term-document matrix as a numpy array, \n",
    "    and weights the scores with the tf-idf algorithm described above.\n",
    "    idf values are modified with log_10\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    td_matrix : numpy array \n",
    "        a matrix where columns are songs and \n",
    "        rows are word counts in a song\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tf_idf_matrix : numpy array \n",
    "        a matrix where columns are songs and \n",
    "        rows are word tf-idf values in a song\n",
    "        \n",
    "    idf_vector : numpy array of shape (vocabulary-size, 1)\n",
    "        a vector of idf values for words in the collection. the shape is (vocabulary-size, 1)\n",
    "        this vector will be used to weight new query documents\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return tf_idf_matrix, idf_vector      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86385c720a1e574d70d3bdc080e67ad4",
     "grade": true,
     "grade_id": "cell-94805d8b0ce03198",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal, assert_allclose, assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_songs = [['la','la','la','la','oh',',','woo',\"uuuuuh\"],\n",
    "               ['oh', 'la','la','la',\"tarara\",'tadada', 'bla', 'bla', 'bla','bla','bla', '!']]\n",
    "\n",
    "\n",
    "sorted_dummy_vocab = ['!', ',', 'bla', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo']\n",
    "correct_td_dummy_matrix = np.array([[0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [0., 5.],\n",
    "                                    [4., 3.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [1., 0.]])\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(tf_idf(correct_td_dummy_matrix)[0].shape, (9, 2))\n",
    "# check the shape of the idf vector\n",
    "assert_equal(tf_idf(correct_td_dummy_matrix)[1].shape, (9, 1))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "# pay attention to the \"la\" row: its values are zeros now. make sure you understand why.  \n",
    "\n",
    "correct_tf_idf_dummy_matrix = np.array([[0.,0.30103],\n",
    "                                        [0.30103,0.],\n",
    "                                        [0.,1.50514998],\n",
    "                                        [0.,0.],\n",
    "                                        [0.,0.],\n",
    "                                        [0.,0.30103],\n",
    "                                        [0.,0.30103],\n",
    "                                        [0.30103,0.],\n",
    "                                        [0.30103,0.]])\n",
    "\n",
    "correct_idf_dummy_vector = np.array([[0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.     ],\n",
    "                                     [0.     ],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103]])\n",
    "\n",
    "assert_allclose(tf_idf(correct_td_dummy_matrix)[0], correct_tf_idf_dummy_matrix, rtol=1e-3)\n",
    "assert_allclose(tf_idf(correct_td_dummy_matrix)[1], correct_idf_dummy_vector, rtol=1e-3)\n",
    "\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA\n",
    "# check the first value of the idf vector\n",
    "assert_almost_equal(tf_idf(td_matrix)[1][0][0], 0.8816918422907132, 3)\n",
    "# check the last value of the idf vector\n",
    "assert_almost_equal(tf_idf(td_matrix)[1][-1][0], 1.99563519459755, 3)\n",
    "# check the [0][0] element of the tf-idf matrix\n",
    "assert_equal(tf_idf(td_matrix)[0][0][0], 0)\n",
    "# check the [0][-1] element of the tf-idf matrix\n",
    "assert_almost_equal(tf_idf(td_matrix)[0][0][-1], 3.5267673691628527, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2708da0fedfeab894377b1ab272a6af5",
     "grade": false,
     "grade_id": "cell-b9180e4bcb31083e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.2 <a class=\"anchor\" id=\"subtask_3_2\"></a>\n",
    "### PPMI (3 points)\n",
    "\n",
    "An alternative weighting scheme to $tf-idf$ is **Positive Pointwise Mutual Information** (PPMI). When using PPMI scheme, we are asking how more frequently a word occurs in a document than it would have been expected to appear there by chance. The more unexpected this occurrence is, the more weight is given. For example, we might expect words like *run* and *boy* to appear in many document, but words like *kerning* or *typeface* to appear only in texts about typography.\n",
    "\n",
    "**PPMI** is a variation of **Pointwise Mutual Information (PMI)**, in which all negative PMI values are replaced with zeros. \n",
    "\n",
    "The mutual information compares the probability of observing two events $x$ and $y$ together to the probability of observing $x$ and $y$ independently. Based on this definition of mutual information, PMI between a term $t$ and a document $d$ is defined in the following way:\n",
    "\n",
    "$PMI_{t,d} = \\log_{2}(\\frac{P(t,d)}{P(t)P(d)})$\n",
    "\n",
    "In the case of term-document matrix, the numerator $P(t,d)$ denotes how often a term $t$ was observed in a document $d$, the denominator $P(t)P(d)$ expresses how often we would see a term $t$ in a document $d$ if their probabilities were independent of each other. The larger the numerator, the more weight the pair gets because it means the word occurs in this document more often than by a pure chance.\n",
    "\n",
    "Let's look at it a bit closer:\n",
    "- $N$ - total counts of all the words in the collection of documents\n",
    "- $C_{t,d}$ - count of a term t in a document $d$\n",
    "- $C_t$ - total count of a term t in the whole document collection (the more we've seen it, the more likely it is to see it again)\n",
    "- $C_d$ - total count of all words in a document $d$ (the length of a document - the more words a document has, the more likely for it to include the word we're interested in)\n",
    "- $PMI_{t,d} = \\log_{2}(\\frac{\\frac{C_{t,d}}{N}}{\\frac{C_t}{N}\\frac{C_d}{N}})$\n",
    "\n",
    "$PPMI_{t,d} = \\max(PMI_{t,d},0)$\n",
    "\n",
    "### How to weigh a query document with PPMI?\n",
    "We are again in the same situation as with tf-idf: the new document vector should have the same number of elements (dimensions) as a vector in our collection, its dimensions should correspond to the dimension of our collection vectors, and the values should be comparable. To be able to weigh the frequencies in the new documents, we are going to need the $C_t$ statistic and $N$ to come **from the collection**. The $C_{t,d}$ and $C_d$ values would come **from the query vector**.\n",
    "\n",
    "#### task description\n",
    "\n",
    "Write a function that can weight with **PPMI** scheme both train and test term-document matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8a84a3792c9426497520edee95d0dd8",
     "grade": false,
     "grade_id": "cell-1758b0ed2838db07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ppmi(td_matrix, query_matrix = None):\n",
    "    \"\"\"Weighs a term-document matrix of raw counts with the PPMI scheme\n",
    "    \n",
    "    this function takes in a term-document matrix as a numpy array, \n",
    "    and weights the scores with the PPMI scheme described above\n",
    "    use PMI values are modified with log_2\n",
    "    if term-document matrix for songs of a test set is given as well,\n",
    "    the functions returns only PPMI weighted test set (query) matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    td_matrix : numpy array \n",
    "        a matrix where columns are collection songs and \n",
    "        rows are word counts in a song\n",
    "    query_matrix : numpy array\n",
    "        a numpy array where columns are query songs and \n",
    "        rows are word counts in a song\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ppmi_matrix - a numpy array \n",
    "        a matrix where columns are songs and \n",
    "        rows are ppmi word values in a song\n",
    "    \"\"\"\n",
    "    if query_matrix is None:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d22b607e8997bb58a8ee8a472ac88563",
     "grade": true,
     "grade_id": "cell-21dee6aeac2b6a25",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal, assert_allclose\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_songs = [['la','la','la','la','oh',',','woo',\"uuuuuh\"],\n",
    "               ['oh', 'la','la','la',\"tarara\",'tadada', 'bla', 'bla', 'bla','bla','bla', '!']]\n",
    "\n",
    "\n",
    "sorted_dummy_vocab = ['!', ',', 'bla', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo']\n",
    "correct_td_dummy_matrix = np.array([[0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [0., 5.],\n",
    "                                    [4., 3.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [1., 0.]])\n",
    "\n",
    "td_dummy_query1 = np.array([[0.],\n",
    "                           [1.],\n",
    "                           [8.],\n",
    "                           [1.],\n",
    "                           [0.],\n",
    "                           [0.],\n",
    "                           [1.],\n",
    "                           [2.],\n",
    "                           [4.]])\n",
    "\n",
    "\n",
    "\n",
    "td_dummy_query2 = np.array([[0., 2., 2.],\n",
    "                           [1., 3., 2.],\n",
    "                           [8., 4., 2.],\n",
    "                           [1., 2., 2.],\n",
    "                           [0., 5., 2.],\n",
    "                           [0., 0., 2.],\n",
    "                           [1., 0., 2.],\n",
    "                           [2., 0., 2.],\n",
    "                           [4., 0., 2.]])\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(ppmi(correct_td_dummy_matrix).shape, (9, 2))\n",
    "assert_equal(ppmi(correct_td_dummy_matrix, td_dummy_query1).shape, (9, 1))\n",
    "assert_equal(ppmi(correct_td_dummy_matrix, td_dummy_query2).shape, (9, 3))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "# pay attention to the row of \"woo\" and the row of \"tarara\" why are the ppmi values different for diiferent documents? \n",
    "\n",
    "correct_ppmi_dummy_matrix = np.array([[0.        , 0.73696559],\n",
    "                                      [1.32192809, 0.        ],\n",
    "                                      [0.        , 0.73696559],\n",
    "                                      [0.51457317, 0.        ],\n",
    "                                      [0.32192809, 0.        ],\n",
    "                                      [0.        , 0.73696559],\n",
    "                                      [0.        , 0.73696559],\n",
    "                                      [1.32192809, 0.        ],\n",
    "                                      [1.32192809, 0.        ]])\n",
    "\n",
    "assert_allclose(ppmi(correct_td_dummy_matrix), correct_ppmi_dummy_matrix, rtol=1e-3)\n",
    "\n",
    "correct_ppmi_dummy_query1 = np.array([[0.        ],\n",
    "                                     [0.23446525],\n",
    "                                     [0.91253716],\n",
    "                                     [0.        ],\n",
    "                                     [0.        ],\n",
    "                                     [0.        ],\n",
    "                                     [0.23446525],\n",
    "                                     [1.23446525],\n",
    "                                     [2.23446525]])\n",
    "\n",
    "correct_ppmi_dummy_query2 = np.array([[0.        , 1.32192809, 1.15200309],\n",
    "                                   [0.23446525, 1.9068906 , 1.15200309],\n",
    "                                   [0.91253716, 0.        , 0.        ],\n",
    "                                   [0.        , 0.        , 0.        ],\n",
    "                                   [0.        , 1.64385619, 0.15200309],\n",
    "                                   [0.        , 0.        , 1.15200309],\n",
    "                                   [0.23446525, 0.        , 1.15200309],\n",
    "                                   [1.23446525, 0.        , 1.15200309],\n",
    "                                   [2.23446525, 0.        , 1.15200309]])\n",
    "\n",
    "assert_allclose(ppmi(correct_td_dummy_matrix, td_dummy_query1), correct_ppmi_dummy_query1, rtol=1e-3)\n",
    "assert_allclose(ppmi(correct_td_dummy_matrix, td_dummy_query2), correct_ppmi_dummy_query2, rtol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a7f295d1aaf5ad1a58c1f0c5d21d7f1",
     "grade": false,
     "grade_id": "cell-ed45ea1e6f004fce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's weigh our term-dictionary matrix\n",
    "Run the cell below to collect the weighted statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95a127128656011969846600e19ed69d",
     "grade": false,
     "grade_id": "cell-0613026c7e3835cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "tf_idf_matrix, idf_vector = tf_idf(td_matrix)\n",
    "\n",
    "# PPMI\n",
    "ppmi_matrix = ppmi(td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b39d1e8ad4b9fd1a9afce78fd32d329",
     "grade": false,
     "grade_id": "cell-1bd0768d79b94e86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 4 <a class=\"anchor\" id=\"task_4\"></a>\n",
    "## Dense vectors\n",
    "## 4.1<a class=\"anchor\" id=\"subtask_4_1\"></a>\n",
    "### Truncated SVD (LSI) (1 point)\n",
    "\n",
    "Raw counts, tf-idf and PPMI are called **sparse representations**. That means they contain lots of zeros because most of the words never appear in every document.\n",
    "\n",
    "Sparse frequency matrices work well, but keeping all vector components can be very computationally expensive in some tasks, moreover, such frequency matrices can take up a lot of storage space. But more importantly, sparse vectors dedicate different dimensions for very similar words like *car* and *automobile* with no modeled relationship between the dimensions. It would be useful to model such words in the same dimension. Such low-dimensional **dense representations**, vectors can be obtained by performing dimensionality reduction on sparse vectors.\n",
    "\n",
    "One of the methods to transform a sparse VSM into a dense VSM is based on **Truncated Singular Value Decomposition (SVD)**. **SVD** is a method for ranking the dimensions of a dataset by their \"importance\". The dimensions along which the data varies the most are considered more important than the dimensions with low variation. **Truncated** means keeping only $k$ most important dimensions out of all.\n",
    "\n",
    "We can approximate our term-document matrix with one of lower rank using the **Truncated SVD**. This approximation matrix yields a new representation for each document in the collection. We will need to transform our query documents into this low-rank representation as well, enabling us to compute query-document similarity scores in this low-rank representation. This process is known as **Latent Semantic Indexing (LSI)**.\n",
    "\n",
    "#### Method description \n",
    "\n",
    "Let's say we have some term-document matrix $X$ with 7 words and 5 documents (7x5 matrix). We want to represent each of these five document vectors with vectors of only 2 elements, i.e turn matrix $X$ into a 2x5 matrix. And we want these elements to correspond to some hidden (latent) topics. Here is how we do it with LSI.\n",
    "1. Produce SVD  of $X$: $X = TSD^T$\n",
    "\n",
    "where\n",
    "* $T$ - an SVD term matrix of 7x7 dimension (rows are eigenvectors of $X^TX$). $X^TX$ expresses relationships between words: if documents $i$ and $j$ have $c$ words in common then $T[i, j]$ = $c$.\n",
    "* $S$ - a diagonal matrix of the singular values with 7x5 dimension, obtained as square roots of $X^TX$ eigenvalues. You can think of it as a weight to apply to different dimensions of our data.\n",
    "* $D^T$ - an SVD document matrix of 5x5 dimension (columns are eigenvectors of $XX^T$). $XX^T$ expresses relationships between documents: if terms $i$ and $j$ occur together in $c$ documents then $D^T[i, j]$ = $c$.\n",
    "\n",
    "2. Truncate $T$, $S$ and $D^T$ matrices in your SVD: implement a rank 2 approximation by keeping the first two columns of $T$, both 2 first columns and two first rows of $S$ and first two rows of $V^T$.\n",
    "\n",
    "3. The rank 2 approximation of $T$ ($T_2$) is a 7x2 matrix that can be interpreted as word assignment to 2 hidden (latent) topics. The rank 2 approximation of $D^T$ ($D_2^T$) is a 2x5 matrix that can be interpreted as document assignment to 2 hidden (latent) topics. The rank 2 approximation of $S$ ($S_2$) is a 2x2 diagonal matrix with \"weights\" for each of the two hidden topics. When we multiply $S_2$ and $D_2^T$ we get our 2x5 dense document vectors!\n",
    "\n",
    "\n",
    "\n",
    "Note: if you do the same thing but with word vectors and $T$ matrix, you'll get representations for words instead of topics. This approach is called Latent Semantic Analysis (LSA). \n",
    "### How to transform a query with SVD?\n",
    "\n",
    "When we need to transform a query document $d$ to the same dense document space, we need to multiply its sparse vector by $T_2S_2^{-}$ (because $D=X^TTS^{-}$). And we get: $d_2 = d^TT_2S_2^{-}$.\n",
    "\n",
    "If this explanation was not enough for you, you can find good tutorials on SVD and LSI [here](https://www.engr.uvic.ca/~seng474/svd.pdf) or [here](https://digilander.libero.it/b.dellavecchia/documents/SVD%20and%20LSI%20Tutorial%204.pdf)\n",
    "\n",
    "#### task description \n",
    "Write a function, that performs truncated SVD on a term-document matrix, and returns $D_k^T$, and $T_kS_k^{-}$ matrices. You just need to truncate the matrices out of SVD, SVD is already done for you.\n",
    "\n",
    "HINT1: $s$ in `np.linalg.svd()` is not a matrix, but a vector of diagonal values of $S$\n",
    "\n",
    "HINT2: use `np.linalg.inv()` to get the inverse of $S_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebfa81f136cf30d47d333dae9c06edbc",
     "grade": false,
     "grade_id": "cell-21690d2849be7b9c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lsi(matrix, d):\n",
    "    \"\"\" Returns truncted SVD components\n",
    "    \n",
    "    this function takes in a term-document matrix, where\n",
    "    values can be both raw frequencies and weighted values (tf_idf, ppmi)\n",
    "    and returns their trunctaded SVD matrices.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        a numpy array where columns are songs and \n",
    "        rows are lemmas\n",
    "    d : int\n",
    "        a number of features we will be reducing our matrix to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DT_d : numpy array\n",
    "        a [d x m], where m is the number of word dimensions in the original matrix, \n",
    "        and d is the number of features we want to keep\n",
    "        this is a matrix that represents documents with values for d hidden topics\n",
    "    transformation_matrix : numpy array \n",
    "        a matrix to transform queries into the same vector space as DT_d\n",
    "        T_dS_d^-, where S_d^- is inverse of S_d\n",
    "    \"\"\"\n",
    "    # Singular-value decomposition is already done for you\n",
    "    T, s, DT = np.linalg.svd(matrix)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return DT_d, transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4673c98008957bc7cdb7c9298402e360",
     "grade": true,
     "grade_id": "cell-b985c3fe7e7fefea",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal, assert_allclose\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_songs = [['la','la','la','la','oh',',','woo',\"uuuuuh\"],\n",
    "               ['oh', 'la','la','la',\"tarara\",'tadada', 'bla', 'bla', 'bla','bla','bla', '!']]\n",
    "\n",
    "\n",
    "sorted_dummy_vocab = ['!', ',', 'bla', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo']\n",
    "correct_td_dummy_matrix = np.array([[0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [0., 5.],\n",
    "                                    [4., 3.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [1., 0.]])\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrices\n",
    "assert_equal(lsi(correct_td_dummy_matrix, 1)[0].shape, (1,2))\n",
    "assert_equal(lsi(correct_td_dummy_matrix, 1)[1].shape, (9,1))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "dummy_query_vector_from_train = correct_td_dummy_matrix[:,0].reshape((9,1)) # take a document identical to the first one in training\n",
    "dummy_query_vector_new = np.arange(9).reshape(9,1) # take a new document \n",
    "dummy_query_matrix = np.concatenate((dummy_query_vector_from_train, dummy_query_vector_new),axis=1) # concatenate queries\n",
    "\n",
    "# check that the LSI matrices are what they should be\n",
    "# documets\n",
    "assert_allclose(lsi(correct_td_dummy_matrix, 1)[0], np.array([[0.46410668, 0.88577931]]), rtol=1e-3)\n",
    "# transormation matrix\n",
    "assert_allclose(lsi(correct_td_dummy_matrix, 1)[1], np.array([[0.01976683],\n",
    "                                                               [0.01035689],\n",
    "                                                               [0.09883417],\n",
    "                                                               [0.10072807],\n",
    "                                                               [0.03012373],\n",
    "                                                               [0.01976683],\n",
    "                                                               [0.01976683],\n",
    "                                                               [0.01035689],\n",
    "                                                               [0.01035689]]), rtol=1e-3)\n",
    "\n",
    "                \n",
    "# check how queries are transformed\n",
    "dummy_query_matrix_dense = (dummy_query_matrix.T.dot(lsi(correct_td_dummy_matrix, 1)[1])).T\n",
    "### see how the first document is the identical to the first document in DT! \n",
    "### that means we indeed transform to the same vector space\n",
    "assert_allclose(dummy_query_matrix_dense, np.array([[0.46410668, 1.00349291]]), rtol=1e-3)\n",
    "\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA\n",
    "# check that the shapes are fine\n",
    "assert_equal(lsi(td_matrix, 2)[0].shape, (2, td_matrix.shape[1]))\n",
    "assert_equal(lsi(td_matrix, 2)[1].shape, (td_matrix.shape[0], 2))\n",
    "# check that the value of TD[0][0] is correct\n",
    "assert_almost_equal(lsi(td_matrix, 2)[0][0][0], -0.0547597353058567, 3)\n",
    "# check that the value of transformation_matrix[0][0] is correct\n",
    "assert_almost_equal(lsi(td_matrix, 2)[1][0][0], -3.1685697212181624e-05, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fee3aef58b3dbd950ef3baa228c59e23",
     "grade": false,
     "grade_id": "cell-d3dd9b50dd7cb7ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's get the dense representations!\n",
    "Well, now we have three matrices: raw counts, tf-idf and ppmi. Let's get their dense representations, by performing LSI on them. We will also collect their transformation matrices here, so that we can use them later for the queries. Run the cell below to get dense 2d vector models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09c8b4e15c674b3264d8dee96d16183f",
     "grade": false,
     "grade_id": "cell-8a824d02b1343ab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "td_matrix_dense, td_matrix_transform = lsi(td_matrix,2)\n",
    "tf_idf_matrix_dense, tf_idf_matrix_transform = lsi(tf_idf_matrix,2)\n",
    "ppmi_matrix_dense, ppmi_matrix_transform = lsi(ppmi_matrix,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a76b405b6f04c2702fef6a4903b2861b",
     "grade": false,
     "grade_id": "cell-20027e4490c61b23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's visualize our models! \n",
    "When we have 2-D representations of songs, we can actually plot them and see if they make sense. Remember, we want songs bu the same artist be close to each other. Can you predict which model out of these 3 will perform better? Which artist would be easier to recognize correctly?\n",
    "\n",
    "Note: We'll remove some outliers, to get a clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95702e4e3167070c8f2cf509395f8495",
     "grade": false,
     "grade_id": "cell-a043812a6145d6c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_songs(song_matrix, title, songs_index, indices_to_remove):\n",
    "    \"\"\" Plots 2d vectors of songs and marks song with an artist label\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    song_matrix : numpy array\n",
    "        columns are songs\n",
    "        rows a latent topics\n",
    "    title : str\n",
    "        title for the plot\n",
    "    songs_index : dict\n",
    "        dictionary of artist and their songs indices in the song_matrix\n",
    "    indices_to_remove : list\n",
    "        list of songs to not plot\n",
    "    \"\"\"\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    pulp_indices = [i for i in songs_index['pulp'] if i not in indices_to_remove]\n",
    "    princess_nokia_indices = [i for i in songs_index['princess_nokia'] if i not in indices_to_remove]\n",
    "    at_the_drive_in_indices = [i for i in songs_index['at_the_drive_in'] if i not in indices_to_remove]\n",
    "    \n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "\n",
    "    pulp = plt.scatter(song_matrix[0,pulp_indices], song_matrix[1,pulp_indices], marker=\"x\", color=\"blue\")\n",
    "    princess_nokia = plt.scatter(song_matrix[0,princess_nokia_indices], song_matrix[1,princess_nokia_indices], marker=\"o\", color=\"red\")\n",
    "    at_the_drive_in = plt.scatter(song_matrix[0,at_the_drive_in_indices], song_matrix[1,at_the_drive_in_indices], marker=\"^\", color=\"black\")\n",
    "    \n",
    "    plt.legend((pulp, princess_nokia, at_the_drive_in),('Pulp','Princess Nokia','At the Drive In'))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_songs(td_matrix_dense, \"Songs as 2-D vectors (raw counts)\", normalized_songs_index_train, [])\n",
    "plot_songs(tf_idf_matrix_dense, \"Songs as 2-D vectors (tf-idf)\", normalized_songs_index_train, [13, 26])\n",
    "plot_songs(ppmi_matrix_dense, \"Songs as 2-D vectors (PPMI)\", normalized_songs_index_train, [31, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f710edf73af30a7979c4cd3ee3989435",
     "grade": false,
     "grade_id": "cell-3bf052727116e4b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 5 <a class=\"anchor\" id=\"task_5\"></a>\n",
    "## Place queries into the right vector space\n",
    "\n",
    "Now we have new songs coming into our system and we would like to know what artist they were written by.\n",
    "The first thing to do is to adjust our query documents to the way we represent the songs in our systems.\n",
    "\n",
    "1. Raw counts: pre-process, collect counts\n",
    "2. Tf-idf: pre-process, collect counts, weigh\n",
    "3. PPMI: pre-process, collect counts, weigh\n",
    "4. LSI of Raw counts: pre-process, collect counts, LSI transform\n",
    "5. LSI of Tf-idf: pre-process, collect counts, weigh, LSI transform\n",
    "6. LSI of PPMI: pre-process, collect counts, weigh, LSI transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecb06897a72a1913e9879e69f1960f3e",
     "grade": false,
     "grade_id": "cell-00f4b36ee58a55ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1: Read and Normalize songs\n",
    "the first step would be to just collect the lemmas for songs in test set using the same pre-processing pipeline as we were using for the training set. Run the cell below to collect the statistics. Make sure you haven't used the name of the stopword list, to not get different results.\n",
    "\n",
    "* `normalized_songs_test` - stores the pre-processed song from the test set\n",
    "* `normalized_songs_index_test` - record which artist the test songs belong to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5312de97cfa24e0e930bb79b6e42fafa",
     "grade": false,
     "grade_id": "cell-5792cf1ed1b62a8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# collect the paths to the albums and songs\n",
    "test_songs = glob.glob('/coursedata/03_vsms/songs_test/*/*')\n",
    "\n",
    "normalized_songs_index_test  = {'pulp':None, 'princess_nokia':None, 'at_the_drive_in':None}\n",
    "normalized_songs_test = []\n",
    "\n",
    "for i, song_path in enumerate(test_songs):\n",
    "    \n",
    "    normalized_songs_test.append(tokenize_and_normalize(song_path, stop_words_english))\n",
    "    \n",
    "    if 'pulp' in song_path:\n",
    "        normalized_songs_index_test = add_song(normalized_songs_index_test,'pulp', i)\n",
    "    \n",
    "    if 'princess_nokia' in song_path:\n",
    "        normalized_songs_index_test = add_song(normalized_songs_index_test,'princess_nokia',i)\n",
    "     \n",
    "    if 'at_the_drive_in' in song_path:\n",
    "        normalized_songs_index_test = add_song(normalized_songs_index_test,'at_the_drive_in',i)\n",
    "        \n",
    "        \n",
    "# some tests to see that things are as they should         \n",
    "assert_equal(len(normalized_songs_test), 34)\n",
    "assert_equal(len(normalized_songs_index_test['pulp']), 14)\n",
    "assert_equal(len(normalized_songs_index_test['princess_nokia']), 9)\n",
    "assert_equal(len(normalized_songs_test[0]), 131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb7a7dfbe315fe4e1db4c9ffb5ead5c3",
     "grade": false,
     "grade_id": "cell-a82aeec9edbeda4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5.1 <a class=\"anchor\" id=\"task_4\"></a>\n",
    "### Step 2: Term-document matrix for queries (3 points)\n",
    "\n",
    "Well, this part is a bit tricky. There might be new, unseen words in our queries, and if we count them, we just won't be able to compare our new documents to our old documents (different words = different dimensions = different vector spaces). To avoid this problem, we will need to only count the things we've seen in the training corpus. Luckily, we've collected the sorted vocabulary!\n",
    "\n",
    "Modify the `create_term_doc_matrix()` function, so that it takes a query document, an old sorted vocabulary, and counts only the words that are in our train vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086264dfd79208e69401d2533af9fbcb",
     "grade": false,
     "grade_id": "cell-34b529f25aacbca7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_term_doc_matrix_queries(normalized_queries, sorted_vocabulary):\n",
    "    \"\"\" Constructs a frequency term-document matrix for queries\n",
    "    \n",
    "    this function takes in a list of songs and a vocabulary list and returns a term-document matrix\n",
    "    the rows are lemma types as given in vocabulary, the columns are songs \n",
    "    the rows should be in the same order as in vocabulary given\n",
    "    the order of the columns should be preserved as it's given in normalized_queries\n",
    "    the cell values are a number of times a lemma was seen in a song\n",
    "    the value should be zero, if a lemma is absent from a song\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalized_queries : a list of lists of strings [['a','a','b','d'], ['a','b','c']]\n",
    "        a list of songs represented as a list of lemmas\n",
    "    sorted_vocabulary : list of strings\n",
    "        a list of all the lemma types used in all training songs (the rows of our matrix)\n",
    "        the words are strings sorted alphabetically\n",
    "        for our example it will be ['a','b','c']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    query_matrix : numpy array\n",
    "        a matrix where columns are songs in normalized_queries \n",
    "        and rows are lemma types from sorted_vocabulary.\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        'd' is not included in the matrix, becaus it is absent from sorted_vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return query_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01109b0ed01cdffe5bbfde158abfe5c7",
     "grade": true,
     "grade_id": "cell-b551ba0ab4efc799",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sorted_dummy_vocab = ['!', ',', 'bla', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo']\n",
    "new_dummy_songs = [['la','oh',',','pada',\"uuuuuh\"],\n",
    "                   ['toot','toot']]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_term_doc_matrix_queries(new_dummy_songs, sorted_dummy_vocab).shape, (9, 2))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_td_new_dummy_matrix = np.array([[0., 3.],\n",
    "                                        [1., 0.],\n",
    "                                        [0., 2.],\n",
    "                                        [3., 3.],\n",
    "                                        [1., 1.],\n",
    "                                        [0., 1.],\n",
    "                                        [0., 1.],\n",
    "                                        [1., 0.],\n",
    "                                        [1., 0.]])\n",
    "\n",
    "assert_array_equal(create_term_doc_matrix_queries(new_dummy_songs, sorted_dummy_vocab),np.array([[0., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [0., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [0., 0.],\n",
    "                                                                                                [0., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [0., 0.]]))\n",
    "# SANITY CHECK FOR SONG DATA\n",
    "# check the shape\n",
    "assert_equal(create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary).shape, \n",
    "       (len(sorted_vocabulary), len(normalized_songs_test)))\n",
    "# check the X_test[0][0] value\n",
    "assert_equal(create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary)[0][0],0)\n",
    "# check the X_test[0][-3] value\n",
    "assert_equal(create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary)[0][-3],22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58ee898c4f57af6ed0a2d3e583028bd2",
     "grade": false,
     "grade_id": "cell-b310484643910687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3: Get tf-idf counts\n",
    "### Step 4: Get PPMI counts\n",
    "### Steps 5-8 transform matrices with LSI\n",
    "Run the cell below to:\n",
    "* collect term-document matrix for the test set\n",
    "* weigh term-document matrix with tf-idf (with the help of `idf_vector` we've computed before)\n",
    "* weigh term-document matrix with PPMI\n",
    "* transform three matrices above into dense vector spaces with LSI (with the help of transformation matrices we've computed in the LSI step)\n",
    "* plot dense vectors for songs in the test set. Does it look OK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90e598716335a1b4cc3c37ca6f66b4b0",
     "grade": false,
     "grade_id": "cell-722616992bbe0e77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# step 2 collect term-document matrix\n",
    "td_queries = create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary)\n",
    "# step 3 weigh term-document matrix with tf-idf\n",
    "tf_idf_queries = td_queries*idf_vector \n",
    "# step 4 weigh term-document matrix with PPMI\n",
    "ppmi_queries = ppmi(td_matrix, td_queries)\n",
    "\n",
    "# steps 5-8\n",
    "td_queries_dense = td_queries.T.dot(td_matrix_transform).T\n",
    "tf_idf_queries_dense = tf_idf_queries.T.dot(tf_idf_matrix_transform).T\n",
    "ppmi_queries_dense= ppmi_queries.T.dot(ppmi_matrix_transform).T\n",
    "\n",
    "\n",
    "plot_songs(td_queries_dense, \"Songs as 2-D vectors (raw counts)\", normalized_songs_index_test, [])\n",
    "plot_songs(tf_idf_queries_dense, \"Songs as 2-D vectors (tf-idf)\", normalized_songs_index_test, [])\n",
    "plot_songs(ppmi_queries_dense, \"Songs as 2-D vectors (PPMI)\", normalized_songs_index_test, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6 <a class=\"anchor\" id=\"task_4\"></a>\n",
    "## Cosine similarity\n",
    "Now that we have the documents represented as vectors, we need some measure to be able to compare them to each other. You're going to implement **Cosine similarity**. You can find a more in-depth discussion of why it works in the part 6.4 of this of Jurafsky and Martin book [chapter](https://web.stanford.edu/~jurafsky/slp3/6.pdf). \n",
    "\n",
    "But the simple intuition behind it can be understood from its formula:\n",
    "\n",
    "**Cosine similarity**:\n",
    "\n",
    "$cos(x,y)=\\frac{\\sum_{i=1}^{n}x_iy_i}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\sqrt{\\sum_{i=1}^{n}y_i^2}}$\n",
    "\n",
    "As you can see, the sum in the numerater gets largeer the more large values (or small negative values) two vectors have at the same dimension. The disciminator helps to make two documents of different length compatible (in the case of models with raw counts, the values along different dimensions are larger for longer documents, becasue they just have more words in them). Cosine similarity rectifies it by normalizing the dot product in the numerator by vector lengths.\n",
    "\n",
    "## 6.1 <a class=\"anchor\" id=\"subtask_6_1\"></a>\n",
    "### Compute cosine similarity (1 point)\n",
    "Complete a function that implement the cosine similarity measure in the cell below. When any of the vectors contains only zeros, their similarity is unknown (there are no common words), and your function should output `-inf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1696068cd22179a3fb3a3c3a96a88e",
     "grade": false,
     "grade_id": "cell-b4392408fd5115a1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cosine(doc1,doc2):\n",
    "    \"\"\" Computes cosine similarity between two vectors\n",
    "        \n",
    "    this function takes in two document vectors and computes cosine similarity between them\n",
    "    when any of the vectors contains only zeros, their similarity is unknown (no common words)\n",
    "    your function should output -inf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc1 : numpy array\n",
    "        the first document vector \n",
    "    doc2 : numpy array  \n",
    "        the second document vector \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : float \n",
    "        cosine similarity\n",
    "    \"\"\"\n",
    "    infinity = float('-inf')\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e392f82334f0eb0abdd142471e2ab684",
     "grade": true,
     "grade_id": "cell-51815dda76c169ce",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal, assert_array_equal, assert_allclose\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_x = np.arange(3)\n",
    "dummy_y = np.arange(3,6)\n",
    "dummy_z = np.zeros(3)\n",
    "\n",
    "assert_almost_equal(cosine(dummy_x,dummy_y),0.8854377448471461, 3)\n",
    "assert_almost_equal(cosine(dummy_x,dummy_z),float('-inf'), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce5cb546a651ccecce5078c9e07be74a",
     "grade": false,
     "grade_id": "cell-aedd17a6f54fba0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 7 <a class=\"anchor\" id=\"task_7\"></a>\n",
    "### Quality evaluation\n",
    "\n",
    "We have 6 systems now: raw counts, tf-idf, PPMI, and LSI applied to the first three systems.\n",
    "\n",
    "Let's see which of them works best. We're are going to use as queries previously unseen songs. For every song in our query songs we're going to look at n closest songs from our collection and see how many of those are from the same artist. Then we are going to calculate **Precision**, **Recall**, **Accuracy**, **Error** and **F-measure** to see which system works best based on those.\n",
    "\n",
    "## 7.1 <a class=\"anchor\" id=\"subtask_7_1\"></a>\n",
    "### Find N closest documents to a quesry song (1 point)\n",
    "Write a function that compares new songs to the songs in our collection. For each song, it should give out an index of top N most similar songs from our collection based on their cosine similarity. When a vector in a query has only zeros, the closeness to it should be determined by index of a song from a matrix_collection:\n",
    "1. the closest document for a zero vector has index 0\n",
    "2. the second closest document for a zero vector has index 1 and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "881a6e5e147c1c20d529c2315f636d04",
     "grade": false,
     "grade_id": "cell-1dad28ebe8df8527",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def closest_n_documents(matrix_collection, matrix_queries, n):\n",
    "    \"\"\"Finds N closest documents from a training collection to every song in a test collection\n",
    "    \n",
    "    this function takes in original document collection, new document collection,\n",
    "    computes cosine similarity between documents in old and new collection, \n",
    "    and outputs the list of n-closest documents to each new song\n",
    "    when a vector in a query has only zeros, \n",
    "        the closeness to it should be determined by index of a song from a matrix_collection:\n",
    "        the closest document for a zero vector has index 0\n",
    "        the second closest document for a zero vector has index 1 and so on\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_collection : numpy array\n",
    "        a term-document matrix of songs in training collection\n",
    "        songs are columns\n",
    "    matrix_queries : numpy array\n",
    "        a term-document matrix of query songs\n",
    "        songs are columns\n",
    "    n : int\n",
    "        a number of closest documents to return\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    closest_docs : a list of lists \n",
    "        a list of length equal to the number of songs in a query matrix\n",
    "        each element is, in turn, a list of n imdices of documents in matrix_collection that were closest to the query\n",
    "        for n=2 and query matrix with 3 songs, the out put should look like so [[1,2],[1,2],[1,2]]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return best_cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "715c3085e99b60cf538b987bf7340cc8",
     "grade": true,
     "grade_id": "cell-0cbe6d8fd7344b44",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal, assert_array_equal, assert_allclose\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_collection = np.concatenate((np.arange(15).reshape((5,3)),np.zeros((5,1))), axis=1)\n",
    "dummy_query_1 = np.arange(5).reshape((5,1))\n",
    "dummy_query_2 = np.arange(10).reshape((5,2))\n",
    "dummy_query_3 = np.arange(15).reshape((5,3))\n",
    "dummy_query_4 = np.zeros((5,2))\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the length of the list\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_1, 1)), 1)\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_2, 1)), 2)\n",
    "# check the len of the first element\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_1, 1)[0]), 1)\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_2, 3)[0]), 3)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "closest_vector_id_1 = closest_n_documents(dummy_collection, dummy_query_1, 1)[0][0]\n",
    "assert_equal(closest_vector_id_1, 0)\n",
    "\n",
    "closest_vector_id_2 = closest_n_documents(dummy_collection, dummy_query_1, 4)\n",
    "assert_equal(closest_vector_id_2, [[0, 1, 2, 3]])\n",
    "\n",
    "closest_vector_id_3 = closest_n_documents(dummy_collection, dummy_query_3, 1)\n",
    "assert_equal(closest_vector_id_3, [[0],[1],[2]])\n",
    "\n",
    "closest_vector_id_4 = closest_n_documents(dummy_collection, dummy_query_4, 2)\n",
    "assert_equal(closest_vector_id_4, [[0, 1], [0, 1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbca56063333695b1599bbcf3864f971",
     "grade": false,
     "grade_id": "cell-11622188becf085e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Collect performance statistics for every query song by every model\n",
    "For every out of 6 models we have, the code below collects the closest 7 songs from a train collection for every song from a test collection and stores this informatiob into `closest_songs` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae689abf9d578213f90c640ea673bacd",
     "grade": false,
     "grade_id": "cell-c7018097db516c6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "models = [td_matrix, tf_idf_matrix, ppmi_matrix, \n",
    "          td_matrix_dense, tf_idf_matrix_dense, ppmi_matrix_dense]\n",
    "queries = [td_queries, tf_idf_queries, ppmi_queries,\n",
    "           td_queries_dense, tf_idf_queries_dense, ppmi_queries_dense]\n",
    "\n",
    "closest_songs = [] # a list of closest songs for every song in every VSM model \n",
    "\n",
    "for i in range(len(models)):\n",
    "    closest_songs.append(closest_n_documents(models[i], queries[i], 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f52a41f0ca813c544c1f5e2aa8b1a99d",
     "grade": false,
     "grade_id": "cell-4c1f7140f72388a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 7.2  <a class=\"anchor\" id=\"subtask_7_2\"></a>\n",
    "### Compare models (5 points)\n",
    "\n",
    "\n",
    "Now we've collected \\[closest 7 songs out of training corpus\\] for \\[each model and for each song in our test corpus\\] let's evaluate our models. We will be looking at five measures: \n",
    "\n",
    "1. Precision = $\\frac{tp}{tp + fp}$ (how many right answers from all answers)\n",
    "2. Recall = $\\frac{tp}{tp + fn}$ (how many right answers from all possible right answers)\n",
    "3. Accuracy = $\\frac{tp+tn}{N}$ (how many right artists were chosen and how many wrong artist we NOT chosen, out of all songs in collection)\n",
    "4. Error = $\\frac{fp+fn}{N}$ (how many right artists were NOT chosen, and how many wrong artist we chosen, out of all songs in collection)\n",
    "5. F-measure $\\frac{1}{\\alpha\\frac{1}{P}+(1-\\alpha)\\frac{1}{R}}$\n",
    "$P$ stands for precision and $R$ for recall, while $\\alpha$ controls the weighting between them. If we choose $\\alpha = 0.5$, $F = \\frac{2PR}{P+R}$\n",
    "\n",
    "\n",
    "Here: $tp$ = True Positives, $fp$ = False Positives, $fn$ = False Negatives, $tn$ = True Negatives, $N$ = the size of the collection.\n",
    "\n",
    "Let's unpack those a bit further. Let's say we have a collection of two artists A and B. The songs by A have indices \\[1,2,3\\], the songs by B have indices \\[4,5,6\\]. Now we have a new song coming from artist B, and we look for 3 closest songs to it and find \\[1,5,4\\]\n",
    "\n",
    "* True Positive - an index out of n closest songs belonging to a song by the same artist as a query (5 and 4 in our example)\n",
    "* False Positive - an index out of n closest songs belonging to a song by an artist different from a query (1 in our example)\n",
    "* False Negative - and index of a song by the same artist as a query that was (wrongly) not in the n best list (6 in our example)\n",
    "* True Negative - and index of a song by a different artist that was (rightly) not in the n best list (2 and 3 in out example)\n",
    "* N - the number of all songs in the collection (6 in our example)\n",
    "\n",
    "SO:\n",
    "* Precision = $\\frac{2}{3}$\n",
    "* Recall = $\\frac{2}{3}$\n",
    "* Accuracy = $\\frac{4}{6}$\n",
    "* Error = $\\frac{2}{6}$\n",
    "* F-measure = $\\frac{2*0.67*0.67}{1.33}$, (with alpha being 0.5)\n",
    "\n",
    "\n",
    "In the cell below, create a function that outputs average model scores for each of these 6 metrics. That is: count the results, sum them and divide by the number of songs in the test set. For the F-measure choose $\\alpha = 0.5$. In case you have no True Positives set F-measure to zero to avoid 'division by zero' error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8778604af09b05c0da0d12adcf6f45d6",
     "grade": false,
     "grade_id": "cell-f7595a21186b1afe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_average_results(closest_songs, train_index, test_index):\n",
    "    \"\"\"Computes average metrics for a model based on n closest songs for a test set \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    closest_songs : a list of lists \n",
    "        a list of length equal to the number of songs in a query matrix\n",
    "        each element contain n closest songs from a training collection to that song\n",
    "    train_index : dict {atrist:lits of song indices}\n",
    "        indices of songs in training collection assigned to artists\n",
    "    test_index : dict {atrist:lits of song indices}\n",
    "        indices of songs in test collection assigned to artists\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average_precision: float \n",
    "        average precision based on all test songs\n",
    "    average_recall: float\n",
    "        average recall based on all test songs\n",
    "    average_accuracy: float\n",
    "        average accuracy based on all test songs\n",
    "    average_error: float\n",
    "        average error based on all test songs\n",
    "    average_f_measure: float\n",
    "        average f_measure based on all test songs\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return average_precision, average_recall, average_accuracy, average_error, average_f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6786dd0b676261ba9d01e79b1adf766",
     "grade": true,
     "grade_id": "cell-43fe14d03d7dcadc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_train_index = {'A':[1,2,3],'B':[4,5,6]}\n",
    "dummy_test_index = {'B':[0], 'A':[1]}\n",
    "dummy_closest_songs1 =[[1,5,4]] # one song, two matches\n",
    "dummy_closest_songs2 =[[1,5,4],[4,5,6]] # two songs, no matches for the second song\n",
    "\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check if function outputs 5 values\n",
    "assert_equal(len(compute_average_results(dummy_closest_songs1, dummy_train_index, dummy_test_index)), 5)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_almost_equal(compute_average_results(dummy_closest_songs1, dummy_train_index, dummy_test_index),\n",
    "                  (0.6666666666666666,\n",
    "                     0.6666666666666666,\n",
    "                     0.6666666666666666,\n",
    "                     0.3333333333333333,\n",
    "                     0.6666666666666666), 2)\n",
    "\n",
    "assert_almost_equal(compute_average_results(dummy_closest_songs2, dummy_train_index, dummy_test_index),\n",
    "                  (0.3333333333333333,\n",
    "                     0.3333333333333333,\n",
    "                     0.3333333333333333,\n",
    "                     0.6666666666666666,\n",
    "                     0.3333333333333333), 2)\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA \n",
    "# check f-measure of \n",
    "assert_almost_equal(compute_average_results(closest_songs[0], \n",
    "                                            normalized_songs_index_train, \n",
    "                                            normalized_songs_index_test)[-1],0.181022,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1a256410c36fa25315c916428327eda",
     "grade": false,
     "grade_id": "cell-dff0d488a6969696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's take a look at the results\n",
    "Run the cell below to get a table with result for every VSM models we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca34f4f1e4aa701d9fde304ccd90472f",
     "grade": false,
     "grade_id": "cell-688800942fe8b15b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns =['Precision', 'Recall', 'Accuracy', 'Error', 'F_measure']\n",
    "rows = ['Raw counts', 'tf-ifd', 'PPMI', 'Raw counts dense', 'tf-ifd dense', 'PPMI dense']\n",
    "df = pd.DataFrame(columns=columns , index=rows)\n",
    "\n",
    "for i,song_results in enumerate(closest_songs):\n",
    "    df.loc[rows[i]] = list(compute_average_results(song_results,\n",
    "                                                         normalized_songs_index_train,\n",
    "                                                         normalized_songs_index_test))\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2f7d5121cd86ee4f580216a6b6cd938",
     "grade": false,
     "grade_id": "cell-88cdc5de83c20c80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Look at different artists\n",
    "Run the cell below to get results for each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c55bc9049170b9185dbda08b95c3725",
     "grade": false,
     "grade_id": "cell-64c5f5a5826afac6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "columns =['Precision', 'Recall', 'Accuracy', 'Error', 'F_measure']\n",
    "rows = ['Raw counts', 'tf-ifd', 'PPMI', 'Raw counts dense', 'tf-ifd dense', 'PPMI dense']\n",
    "artists = ['pulp', 'princess_nokia', 'at_the_drive_in']\n",
    "\n",
    "new_columns = []\n",
    "for artist in artists:\n",
    "    for column in columns:\n",
    "            new_columns.append(column+'_'+artist)\n",
    "\n",
    "df = pd.DataFrame(columns=rows , index=new_columns)\n",
    "\n",
    "for i,song_results in enumerate(closest_songs):\n",
    "    model_scores = []\n",
    "    for artist in artists:\n",
    "        artist_indices = normalized_songs_index_test[artist]\n",
    "        song_results_artist = [song_results[i] for i in artist_indices]\n",
    "        index_dict_artist = {artist : [x for x in range(len(artist_indices))]}\n",
    "        model_scores+=list(compute_average_results(song_results_artist, normalized_songs_index_train, index_dict_artist))\n",
    "    df[rows[i]] = model_scores\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc80b6b819d60b01c834ca5d74572333",
     "grade": false,
     "grade_id": "cell-ac1d2fe8616c0d95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 7.3 <a class=\"anchor\" id=\"subtask_7_3\"></a>\n",
    "### Choose the best model (3 points)\n",
    "After looking at the result tables, answer briefly in the cell below: \n",
    "* What model/models performed best? \n",
    "* What do you think is the reason?\n",
    "* What metrics helped you to form this opinion and why?\n",
    "* What artist was the easiest to get right? Why do you think so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1104254778d738f4f3eed8e1160dfa6e",
     "grade": true,
     "grade_id": "cell-8aad45982ff2d3b7",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "265e574671bfb843af9b0bacd5587026",
     "grade": false,
     "grade_id": "cell-31d6bb5e94c1bc20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything. Be careful now (our text pre-processing takes time).\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=699484) section of Mycoures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
