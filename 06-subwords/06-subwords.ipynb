{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbff7869c175537d2c1c464d56d17dc1",
     "grade": false,
     "grade_id": "cell-a74ccc5dcde03d2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 6: Subword segmentation\n",
    "\n",
    "## Released: 09.03.2021 at 21:30\n",
    "## Deadline: 22.03.2020 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbba2a0c6e61089c494b440dc9174910",
     "grade": false,
     "grade_id": "cell-bd0631ed211d7cca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "We've already talked about the problem of segmenting text into appropriate units (tokenization). Back then, it was words that were considered as those units, but there are other units that you should probably explore as well: characters and **subwords**. In this assignment, we're going to focus on **tokenization into subwords**.\n",
    "\n",
    "The motivation to segmenting words further into smaller elements comes from morphology, where such elements are called **morphemes**. A **morpheme** is defined as the smallest meaning-bearing unit of a language. For example, the word *unpredictable* contains three morphemes: *un*, *predict* and *able*. As you can see, many morphemes are not unique to one word, they are elements that are regularly seen in other words too. For example, *un* in *unhappy*, *predict* in *predictive*, and *able* in *comfortable*. Thus, just as sentences are constructed from words, words are constructed from morphemes. \n",
    "\n",
    "Segmenting into morphemes (especially in languages with rich morphology) helps to avoid the problem of out-of-vocabulary (OOV) words in text corpora. For example, if our training corpus contains *cool*, *cool-est* and *dumb-er* when the new word *cooler* comes in, it can be analyzed into *cool-er*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e92be7689663906ea2286ac72f616554",
     "grade": false,
     "grade_id": "cell-5f9b8e119097a2a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Task 1: BPE](#task_1)\n",
    "    * [Step 1.1](#subtask_1_1)\n",
    "        * [Step 1.1.1: Collecting word counts](#subtask_1_1_1)\n",
    "        * [Step 1.1.2: Convering words to characters](#subtask_1_1_2)\n",
    "    * [Step 1.2: Collecting symbol pairs frequencies](#subtask_1_2)\n",
    "    * [Step 1.3: Merging the most frequent pair](#subtask_1_3)\n",
    "    * [Step 1.4: Combining steps 1-3 together](#subtask_1_4)\n",
    "    * [Step 1.5-7: Segmenting a corpus](#subtask_1_5)\n",
    "* [Task 2: ANALYZE SEGMENTATIONS](#task_2)\n",
    "    * [Step 2.1: Count word OOV](#subtask_2_1)\n",
    "    * [Step 2.2: Count subword OOV](#subtask_2_2)\n",
    "    * [Step 2.3: Does the segmentation make sense?](#subtask_2_3)\n",
    "    * [Step 2.4: Your thoughts](#subtask_2_4)\n",
    "* [Checklist](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a0d2edcb06d810cf127c152eefbb714",
     "grade": false,
     "grade_id": "cell-2cbd322d57c4b67a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## BPE\n",
    "One approach to tokenization into subwords is based on the **byte pair encoding** (**BPE**) algorithm for text compression. It iteratively merges frequent pairs of characters forming new subwords. The intuition here is that morphemes are frequently repeated substrings, so this method should merge symbols into them instead of into some random meaningless character sequences. The algorithm is applied only inside words (there is no merges across word boundaries). \n",
    "\n",
    "**BPE** algorithm begins with its vocabulary being a set of characters seen in the training corpus. Each word in the corpus is represented as a sequence of characters plus a special end-of-word symbol '_'. At each iteration step $k_i$, the algorithm counts the number of symbol pairs, finds the most frequent pair ['A','B'] and replaces it with the new merged symbol ‘AB’. The algorithm stops when it's done $k$ merges ($k$ is a parameter of the algorithm). The algorithm begins with the set of symbols equal to the set of characters. The resulting symbol set should have the original set of characters plus $k$ new symbols. \n",
    "\n",
    "To learn segmentations with **BPE**, you should take the following steps:\n",
    "* STEP 1: tokenize a training corpus into words and collect frequency statistics of word tokens in the training corpus. Additionally, represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 2: count the frequencies of symbol pairs.\n",
    "* STEP 3: replace every occurrence of the most frequent pair ['A', 'B'] with the new merged symbol 'AB'.\n",
    "* STEP 4: repeat STEPs 2-3 $k-1$ times more.\n",
    "\n",
    "To segment a test corpus with the learned segmentation, you should:\n",
    "* STEP 5: tokenize a test corpus into words (with the same tokenization algorithm as in training).\n",
    "* STEP 6: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 7: for every word apply each merge operation in the order they were learned, and return the tokenized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a4560f44b39d4596fb668023dbf7a5e",
     "grade": false,
     "grade_id": "cell-fc78b008631ec01d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### STEP 1\n",
    "### 1.1.1  <a class=\"anchor\" id=\"subtask_1_1_1\"></a>\n",
    "### Collecting word counts (1 Point)\n",
    "\n",
    "Write a function that reads a text as one string from a file, tokenizes it into words by whitespaces and collects the word frequencies. It should return a dictionary of words and their raw counts in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "225fcd9a997fff08248949d2d582eae9",
     "grade": false,
     "grade_id": "cell-1b45cebfbb7868fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def collect_word_counts(file_name):\n",
    "    \"\"\"\n",
    "    Takes in a path to a text file, reads the file, splits it into words by whitespaces, \n",
    "    and then counts the words' frequencies.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    file_name : string\n",
    "            a path to a training corpus as a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts : dcitionary\n",
    "            a dictionary of word counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    with open(file_name) as f:\n",
    "        text=f.read()\n",
    "        words=text.split()\n",
    "        f.close()\n",
    "    word_counts=dict(Counter(words))\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc11c24648325ed3a83e7f8976d526e",
     "grade": true,
     "grade_id": "cell-ac8d751900ffa327",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"/coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(collect_word_counts(dummy_corpus_path)), dict)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['low'], 5)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['lowest'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['new'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['newer'], 6)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['WIDer'], 3)\n",
    "\n",
    "\n",
    "# A SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "gum_train_path = \"/coursedata/06-subwords/gum_train.txt\"\n",
    "# check that the vocabulary length is right\n",
    "assert_equal(len(collect_word_counts(gum_train_path)), 8677)\n",
    "# check that the word count is right\n",
    "assert_equal(collect_word_counts(gum_train_path)['we'], 112)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4db9a8174363efdf8678ecb380534f8",
     "grade": false,
     "grade_id": "cell-41847e0b7837e171",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1.2  <a class=\"anchor\" id=\"subtask_1_1_1\"></a>\n",
    "### Converting words to characters (1 Point)\n",
    "Now, represent each word in your frequency dictionary as a tuple of characters plus a special end-of-word marker '_'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a40d6b8a176061a4b790ccdd5d1c7bd",
     "grade": false,
     "grade_id": "cell-e358614034bbeaad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_chars(vocab):\n",
    "    \"\"\"\n",
    "    Takes in a frequeny dictionary of words in the training corpus\n",
    "    and converts the key words to a tuple of characters plus a special end-of-word symbol '_'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : dictionary\n",
    "        a frequency dictionary of words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    separated_vocab : dictionary \n",
    "        a frequency dictionary of words represented as a tuple of characters \n",
    "        plus a special end-of-word symbol '_'\n",
    "        {('l','o','w','_') : 3}\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    separated_vocab={}\n",
    "    for key, value in vocab.items():\n",
    "        split_key=list(key)\n",
    "        split_key.append('_')\n",
    "        separated_vocab[tuple(split_key)]=value\n",
    "    return separated_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a539a8a68bee21b1ed5d78736d65466",
     "grade": true,
     "grade_id": "cell-8efbbf91eee65873",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab = {'low': 5, 'lowest': 2, 'newer': 6, 'wider': 3, 'New': 2}\n",
    "\n",
    "# check that the output of the function is a dict\n",
    "assert_equal(type(convert_to_chars(dummy_freq_vocab)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(convert_to_chars(dummy_freq_vocab).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(convert_to_chars(dummy_freq_vocab)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', '_')], 5)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', 'e', 's', 't', '_')], 2)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('n', 'e', 'w', 'e', 'r', '_')], 6)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('w', 'i', 'd', 'e', 'r', '_')], 3)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('N', 'e', 'w', '_')], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62200ea28884b197c5b2dcb89e890873",
     "grade": false,
     "grade_id": "cell-48322d22d20d67b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "### STEP 2\n",
    "### Collecting symbol pairs frequencies (1 Point)\n",
    "Write a function that takes in a frequency dictionary where keys are words represented as tuples of symbols, and outputs the most frequent pair of symbols in the corpus. In the case, when there are several pairs with the same frequency, return the pair that is earlier alphabetically. \n",
    "\n",
    "For example, if we only have one *l o o k _* and one *l o o p _*  in our frequency dictionary, the fuction should output *l o* as the most frequent pair (it is earlier than *o o* alphabetically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f469bc5c4dbd606fcd5ae0034b4496ad",
     "grade": false,
     "grade_id": "cell-e2a9fb6eb0db7f82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_the_pair_to_merge(vocab_as_symbols):\n",
    "    \"\"\"\n",
    "    Takes in a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts, \n",
    "    and outputs the most frequent pair of symbols in the corpus.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    vocab_as_symbols : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merge_pair : tuple of strings \n",
    "        the most frequent pair of symbols, a pair to merge\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    statistic={}\n",
    "    max_value=0\n",
    "    for key, value in vocab_as_symbols.items():\n",
    "\n",
    "        for index,char in enumerate(key):\n",
    "            if index<len(key)-1:\n",
    "                pair=(char,key[index+1])\n",
    "                if pair in statistic.keys():\n",
    "                    statistic[pair]+=value\n",
    "                else:\n",
    "                    statistic[pair]=value\n",
    "\n",
    "                if statistic[pair]> max_value:\n",
    "                        max_value=statistic[pair]\n",
    "                        merge_pair=pair\n",
    "                elif statistic[pair]== max_value and pair<merge_pair:\n",
    "                    max_value=statistic[pair]\n",
    "                    merge_pair=pair        \n",
    "\n",
    "    \n",
    "    return merge_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61e6b2fdb29cdfc34613a0f1dc275d46",
     "grade": true,
     "grade_id": "cell-74bc7d2d9ec1da10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab_as_symbols = {('l','o','o','k','_') : 1, \n",
    "                               ('l','o','o','p','_') : 1}\n",
    "\n",
    "# check that the output of the function is a tuple\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)), tuple)\n",
    "# check that the output of the function is a tuple of strings\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)[0]), str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols), ('l', 'o'))\n",
    "\n",
    "\n",
    "dummy_freq_vocab_as_symbols2 = {('l', 'o', 'w', '_'): 5,\n",
    "                                ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                                ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                                ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                                ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols2), ('e', 'r'))\n",
    "\n",
    "# check that your functions can merge not only characters\n",
    "\n",
    "dummy_freq_vocab_as_symbols3 = {('lo', 'o', 'k', '_'): 1 , \n",
    "                                ('lo', 'o', 'p','_'): 1}\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols3), ('lo', 'o'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "645af2ff8e65afb8eeade8b239298f45",
     "grade": false,
     "grade_id": "cell-eb3e7bd4a0d2d372",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 <a class=\"anchor\" id=\"subtask_1_3\"></a>\n",
    "### STEP 3\n",
    "### Merging the most frequent pair (3 Points)\n",
    "\n",
    "Write a function that takes in a pair of symbols to merge and a frequency dictionary where words are represented as tuples of symbols, and returns a frequency dictionary, where words are still represented as tuples of symbols, but the most frequnt pair is now merged in every word.\n",
    "\n",
    "\n",
    "For example, if we want to merge *l* and *o* in our *l o o k _* and *l o o p _*  frequency dictionary, the fuction should output *lo o k _* and *lo o p _*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "580f10b6fe27d9d676fcc2abe4716412",
     "grade": false,
     "grade_id": "cell-7f9d1e7549722c1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def merge(vocab_as_symbols, merge_pair):\n",
    "    \"\"\"Merges the most frequent pair of symbols in all words\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    vocab_as_symbols : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts\n",
    "    merge_pair : tuple\n",
    "        a pair of symbols to merge\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    new_vocab : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols, \n",
    "        with the given pair represented as a new symbol (concatenated pair)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    new_vocab={}\n",
    "    \n",
    "    for key, value in vocab_as_symbols.items():\n",
    "        newkey=()\n",
    "        skip_index=-1\n",
    "        for index,char in enumerate(key):\n",
    "            if index==skip_index:\n",
    "                continue\n",
    "            if index<len(key)-1 and (char, key[index+1]) == merge_pair:\n",
    "                newkey+=(merge_pair[0]+merge_pair[1],)\n",
    "                skip_index=index+1\n",
    "            else:\n",
    "                newkey+=(char,)\n",
    "        new_vocab[newkey]=value\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2af8555a63273ff2c604c68cf7880fa9",
     "grade": true,
     "grade_id": "cell-e71ea4c2a2aee8c2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_vocab_as_symbols = {('l', 'o', 'w', '_'): 5,\n",
    "                          ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                          ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                          ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                          ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "dummy_pair = ('e', 'r')\n",
    "\n",
    "# check that the output of the function is a dictionary\n",
    "assert_equal(type(merge(dummy_vocab_as_symbols, dummy_pair)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(merge(dummy_vocab_as_symbols, dummy_pair).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(merge(dummy_vocab_as_symbols, dummy_pair)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the pair was merged everywhere\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols, dummy_pair).keys()), [('l', 'o', 'w', '_'), \n",
    "                                                                    ('l', 'o', 'w', 'e', 's', 't', '_'), \n",
    "                                                                    ('n', 'e', 'w', 'er', '_'), \n",
    "                                                                    ('w', 'i', 'd', 'er', '_'), \n",
    "                                                                    ('n', 'e', 'w', '_')])\n",
    "# check that you can handle several merge pairs in a word\n",
    "dummy_vocab_as_symbols2 = {('l','o','o','o','l'): 1, \n",
    "                           ('l','o','o','o','o','l'):1}\n",
    "\n",
    "dummy_pair2 = ('o', 'o')\n",
    "\n",
    "\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols2, dummy_pair2).keys()), [('l', 'oo', 'o', 'l'), \n",
    "                                                                        ('l','oo','oo','l')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1d084a68e84b418e9e3a6503d02d318",
     "grade": false,
     "grade_id": "cell-ddba734f24516d1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 <a class=\"anchor\" id=\"subtask_1_4\"></a>\n",
    "### STEP 4\n",
    "### Combining steps 1-3 together (1 Point)\n",
    "Now let's combine steps 1-3 into a function that learns $k$ BPE merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "907e03f406a5e007367d14e9b204c448",
     "grade": false,
     "grade_id": "cell-ccfb0b9de81867b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def learn_BPE_merges(file_name, k):\n",
    "    \"\"\" Learns k BPE subwords\n",
    "    \n",
    "    STEP 1: Collect a word count dictionary from a file\n",
    "            represent words as a tuple of their characters plus a special end-of-word symbol '_'\n",
    "    Now k times\n",
    "        STEP 2: Choose the most frequent pair of symbols\n",
    "                add this pair as a new subword unit into a subword vocabulary\n",
    "        STEP 3: Merge the symbols in all words\n",
    "\n",
    "        \n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    file_name : string\n",
    "        a path to a training corpus as a string\n",
    "    k : integer\n",
    "        a number of merges to be learned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merges : list of strings\n",
    "        a list of k subwords (symbol merges) in the order they were learned\n",
    "        one merge is a tuple of two symbols in the most frequent pair at step k\n",
    "    \"\"\"\n",
    "    \n",
    "    merges = []\n",
    "    # YOUR CODE HERE\n",
    "    word_counts=collect_word_counts(file_name)\n",
    "    vocab_as_symbols=convert_to_chars(word_counts)\n",
    "    for _ in range(k):\n",
    "        \n",
    "        merge_pair=get_the_pair_to_merge(vocab_as_symbols)\n",
    "        vocab_as_symbols=merge(vocab_as_symbols, merge_pair)\n",
    "        merges.append(merge_pair)\n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f0850b1c7e27f50581f115fcea5a3a2",
     "grade": true,
     "grade_id": "cell-5bfcbdbe63a3fdff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"/coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)), list)\n",
    "# check that the output of the function is a list of tuples\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0]), tuple)\n",
    "# check that the output of the function is a list of tuples of strings\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0][0]), str)\n",
    "#check that there are exactly k merges\n",
    "assert_equal(len(learn_BPE_merges(dummy_corpus_path, 10)), 10)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that 8 dummy merges are correct\n",
    "assert_equal(learn_BPE_merges(dummy_corpus_path, 8), [('e', 'r'),\n",
    "                                                     ('er', '_'),\n",
    "                                                     ('e', 'w'),\n",
    "                                                     ('n', 'ew'),\n",
    "                                                     ('l', 'o'),\n",
    "                                                     ('lo', 'w'),\n",
    "                                                     ('new', 'er_'),\n",
    "                                                     ('low', '_')])\n",
    "\n",
    "# A SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "gum_train_path = \"/coursedata/06-subwords/gum_train.txt\"\n",
    "assert_equal(learn_BPE_merges(gum_train_path, 5), [('e', '_'),\n",
    "                                                   ('s', '_'),\n",
    "                                                   ('t', 'h'),\n",
    "                                                   ('t', '_'),\n",
    "                                                   ('d', '_')])\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f154e71552f970c6934fd207b41d5e28",
     "grade": false,
     "grade_id": "cell-b15a884d8a45d11c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5-7 <a class=\"anchor\" id=\"subtask_1_5\"></a>\n",
    "### STEPS 5-7\n",
    "### Segmenting a corpus (5 Points)\n",
    "Well, now we can apply what we've learned to tokenize any text into its subwords. Write a function that reads the test corpus and applies the merges we've learned on a training corpus. Note that you will need to adapt your previous functions a little for this. For example, we don't need to count the word frequencies since they don't play any role here anymore.\n",
    "\n",
    "Just a reminder of the steps needed to apply a subword tokenzation to a new (or an old) text:\n",
    "\n",
    "* STEP 5: tokenize a test corpus into words (with the same tokenization algorithm as in training).\n",
    "* STEP 6: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 7: for every word apply each merge operation in the order they were learned, and return the tokenized text.\n",
    "\n",
    "\n",
    "For the purposes of the exercise, the tokenized text should be a list of strings where strings are words with their subwords separated by whitespaces: ['I', 'lo ok', 'g oo d']. This way it will be easier for you to check how each word is tokenized. In the real application, a tokenized text will be represented just as a list of subwords.\n",
    "\n",
    "\n",
    "Note: don't forget to get rid of the special end-of-word symbol after tokenization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f94859ab9c9e366bff7f6dc79a9fd877",
     "grade": false,
     "grade_id": "cell-bc393db75acac1eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def segment_text(file_name, merges):\n",
    "    \"\"\"\n",
    "    Takes in a path to a text file and lerned BPE merges,\n",
    "    reads the file and tokenizes it into subwords in accorance with the merges.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    file_name : string\n",
    "        a path to a text as a string\n",
    "    merges : list of tuples\n",
    "        a list of k merges in the order they were learned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    segmented_text - list of strings\n",
    "        a text segmented with BPE\n",
    "        the text is a list of words\n",
    "        where each word is a string with its segments separated by whitespaces:\n",
    "        ['I', 'lo ok', 'g oo d']\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    segmented_text=[]\n",
    "    with open(file_name) as f:\n",
    "        text=f.read()\n",
    "        words=text.split()\n",
    "        f.close()\n",
    "    \n",
    "    for word in words:\n",
    "        word_list=list(word)\n",
    "        word_list.append('_')\n",
    "        for merge in merges:\n",
    "            new_word_list=[]\n",
    "            skip_index=-1\n",
    "            for index ,char in enumerate(word_list):\n",
    "                if skip_index==index:\n",
    "                    continue\n",
    "                if index < len(word_list)-1 and (char,word_list[index+1])==merge:\n",
    "                    new_word_list.append(char+word_list[index+1])\n",
    "                    skip_index=index+1\n",
    "                else:\n",
    "                    new_word_list.append(char)\n",
    "            word_list=new_word_list\n",
    "        if '_' in word_list:\n",
    "            new_word=' '.join(word_list[:-1])\n",
    "        else:\n",
    "            new_word=' '.join(word_list).split('_')[0]\n",
    "        segmented_text.append(new_word)\n",
    "    return segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60682daf4bbfa5025ae8f6c4c735e83d",
     "grade": true,
     "grade_id": "cell-6d91df8425edb415",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_train_path = \"/coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "dummy_merges = [('e', 'r'),\n",
    "                ('er', '_'),\n",
    "                ('e', 'w'),\n",
    "                ('n', 'ew'),\n",
    "                ('l', 'o'),\n",
    "                ('lo', 'w'),\n",
    "                ('ma', 'ma')]\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(segment_text(dummy_train_path, dummy_merges)), list)\n",
    "# check that the output of the function is a list of strings\n",
    "assert_equal(type(segment_text(dummy_train_path, dummy_merges)[0]), str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the dummy train corpus is segmented the way it should:\n",
    "\n",
    "assert_equal(segment_text(dummy_train_path, dummy_merges), ['low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low e s t',\n",
    "                                                             'low e s t',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'W I D er',\n",
    "                                                             'W I D er',\n",
    "                                                             'W I D er',\n",
    "                                                             'new',\n",
    "                                                             'new'])\n",
    "\n",
    "\n",
    "# check that the dummy test corpus is segmented the way it should:\n",
    "dummy_test_path = \"/coursedata/06-subwords/dummy_test_corpus.txt\"\n",
    "assert_equal(segment_text(dummy_test_path, dummy_merges), ['low er', 'c o o l er'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "826c396efa5ec15159814e2250abddea",
     "grade": false,
     "grade_id": "cell-3b41d5204748e381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## ANALYZE SEGMENTATIONS\n",
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Count word OOV (1 Point)\n",
    "Now that we've done with the algorithm, let's see if it will actually help us with the OOV problem.\n",
    "Let's use the same corpus as we did in the previous POS-tagging assignment. We've randomly shuffled the sentences and split the corpus in roughly half. One half will be our training example, and another will be our test example.\n",
    "\n",
    "Analyze:\n",
    "1. The number of word **types** in the vocabulary of the training corpus.\n",
    "2. The number of word **types** in the vocabulary of the test corpus.\n",
    "3. The percentage of word **types** in the test corpus vocabulary that are absent in the training corpus. (what part of test corpus vocabulary is OOV?)\n",
    "\n",
    "Run the cell below, to collect the tokenized corpora (we're splitting the words in the corpora by whitespaces), type in the answer in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cfff879d293f8960f52f6a9f6c8e777",
     "grade": false,
     "grade_id": "cell-a3ce2430b488bbe3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/coursedata/06-subwords/gum_train.txt\", 'r') as f:\n",
    "    train = f.read()\n",
    "    train = train.split()\n",
    "\n",
    "with open(\"/coursedata/06-subwords/gum_test.txt\", 'r') as f:\n",
    "    test = f.read()\n",
    "    test = test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n",
      "8770\n",
      "58.026967846029734\n"
     ]
    }
   ],
   "source": [
    "set1=set(train)\n",
    "set2=set(test)\n",
    "print(len(set1))\n",
    "print(len(set2))\n",
    "print((len(set2)-len(set1.intersection(set2)))/(len(set1))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdbed017e21685e1461c6d5b91ee05bf",
     "grade": false,
     "grade_id": "cell-6b5793a004cb3f7c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_train_vocab = 234\n",
    "len_of_train_vocab = 8677\n",
    "\n",
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_test_vocab = 234\n",
    "len_of_test_vocab = 8770\n",
    "\n",
    "# type in the answer as a float number between 0 and 100\n",
    "# For example:\n",
    "# oov_percentage = 90.9\n",
    "oov_percentage= 58.03\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c09d88df0a79688c9a83b55d856aa9e",
     "grade": true,
     "grade_id": "cell-dc2b01fa475e484e",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_train_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(7000 < len_of_train_vocab < 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53f043c15470ae2bf5f88655d3aa2208",
     "grade": true,
     "grade_id": "cell-f522e06e0c3478c1",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_test_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(7000 < len_of_test_vocab < 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0f33f040d534dd518a9f8ecce86b42f",
     "grade": true,
     "grade_id": "cell-1223f6fd86f2005c",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is a float\n",
    "assert_equal(type(oov_percentage), float)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(1 < oov_percentage < 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cc1d44470ad292deddf787063e9d27c",
     "grade": false,
     "grade_id": "cell-586822c916ee5799",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 <a class=\"anchor\" id=\"subtask_2_2\"></a>\n",
    "### Count subword OOV (1 Point)\n",
    "\n",
    "now Let's compare the OOV numbers we've got in the case where the text is tokenized by words and the case when it's tokenized by subwords. \n",
    "\n",
    "Learn 5000 BPE segmentation from the training data, then segment both corpora and compare the vocabulary numbers again. Note that it will take a couple of minutes to run BPE.\n",
    "\n",
    "Analyze:\n",
    "1. The number of subword **types** in the vocabulary of the training corpus.\n",
    "2. The number of subword **types** in the vocabulary of the test corpus.\n",
    "3. The percentage of subword **types** in the test corpus vocabulary that are absent in the training corpus. (what part of test corpus vocabulary is OOV?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a69503220dc9f4e9320c6e1821ddc46a",
     "grade": false,
     "grade_id": "cell-10da1fe631f3755b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "merges = learn_BPE_merges(\"/coursedata/06-subwords/gum_train.txt\", 5000)\n",
    "segmented_train = segment_text(\"/coursedata/06-subwords/gum_train.txt\", merges)\n",
    "segmented_test = segment_text(\"/coursedata/06-subwords/gum_test.txt\", merges)\n",
    "# represent texts as a list of subwords\n",
    "segmented_train_as_subwords = \" \".join(segmented_train).split(\" \")\n",
    "segmented_test_as_subwords = \" \".join(segmented_test).split(\" \")\n",
    "\n",
    "\n",
    "# double checking that your BPE algorithm is working correctly\n",
    "assert_equal(merges[0], ('e', '_'))\n",
    "assert_equal(merges[1000], ('t', 'ro'))\n",
    "assert_equal(merges[-1], ('des', 'cendants_'))\n",
    "\n",
    "assert_equal(segmented_train[580], 'V ill age')\n",
    "assert_equal(segmented_train[5400], 'laun ching')\n",
    "\n",
    "assert_equal(segmented_test[501], 'C a the dr al')\n",
    "assert_equal(segmented_test[517], 'ho t els')\n",
    "\n",
    "assert(len(segmented_train_as_subwords) == 64166)\n",
    "assert(len(segmented_test_as_subwords) == 72712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4281\n",
      "3595\n",
      "3.1067507591684187\n"
     ]
    }
   ],
   "source": [
    "set1=set(segmented_train_as_subwords)\n",
    "set2=set(segmented_test_as_subwords)\n",
    "print(len(set1))\n",
    "print(len(set2))\n",
    "print((len(set2)-len(set1.intersection(set2)))/(len(set1))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "935cdbba973ce3c0b534dc2d32691eef",
     "grade": false,
     "grade_id": "cell-e5d749c71ee045a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as an integer number\n",
    "# For example:\n",
    "# len_of_train_sub_vocab = 234\n",
    "len_of_train_sub_vocab = 4281\n",
    "\n",
    "# type in the answer as an integer number\n",
    "# For example:\n",
    "# len_of_test_sub_vocab = 234\n",
    "len_of_test_sub_vocab = 3595\n",
    "\n",
    "# type in the answer as a float number between 0 and 100\n",
    "# For example:\n",
    "# oov_sub_percentage = 50.9\n",
    "oov_sub_percentage  = 3.11\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4149bad7495bb75d31814dc3b55700b",
     "grade": true,
     "grade_id": "cell-c29d08ffc3d29073",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_train_sub_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(500 < len_of_train_sub_vocab < 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75507a3bc824971b3b98d08fb2091ecf",
     "grade": true,
     "grade_id": "cell-786b076f34bd1f3f",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_test_sub_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(500 < len_of_test_sub_vocab < 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "989dd2107af87f96c82c8d1a2dfd690f",
     "grade": true,
     "grade_id": "cell-3d0f1cd7e311c960",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks if your answer is a float\n",
    "assert_equal(type(oov_sub_percentage), float)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(1 < oov_sub_percentage < 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb07596827b30c3df8ae3e9e1dea8535",
     "grade": false,
     "grade_id": "cell-e9c3e0cf6893442b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.3 <a class=\"anchor\" id=\"subtask_2_3\"></a>\n",
    "### Does the segmentation make sense? (1 Point)\n",
    "Now let's look a bit closer at the subwords that we've learned. Take a second to think if the results are what you would expect them to be.\n",
    "\n",
    "1. What are the top 10 most frequent subwords in the segmented test corpus? (in decending frequency order)\n",
    "2. What are the top 5 longest subwords in the segmented test corpus? (sort alphabetically)\n",
    "3. What are the top 5 most frequent lengths of subwords in the test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b72f4d3acf0ed7fbfaf448f35349083",
     "grade": false,
     "grade_id": "cell-e30eb3fbd26f4157",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as a list of strings\n",
    "# For example:\n",
    "# top_10_by_freq = ['a','b'...]\n",
    "top_10_by_freq = ['the',',','.','of','and','a','in','to','on','is']\n",
    "\n",
    "# type in the answer as a list of strings\n",
    "# For example:\n",
    "# top_5_by_len = ['a','b'...]\n",
    "top_5_by_len = ['entertainment', 'international', 'unprecedented', 'representative', 'transportation']\n",
    "\n",
    "# type in the answer as a list of integers\n",
    "# For example:\n",
    "# top_5_freqs_of_lens= [1,2,3...]\n",
    "top_5_freqs_of_lens = [2, 1, 3, 4, 5]\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2456), (',', 2295), ('.', 2249), ('of', 1356), ('and', 1301), ('a', 1248), ('in', 1138), ('to', 1075), ('on', 527), ('is', 522)]\n",
      "['specifically', 'International', 'Revolutionary', 'approximately', 'concentration', 'entertainment', 'international', 'unprecedented', 'representative', 'transportation']\n",
      "[(2, 19569), (1, 17039), (3, 16870), (4, 9100), (5, 4412)]\n"
     ]
    }
   ],
   "source": [
    "counts=Counter(segmented_test_as_subwords)\n",
    "print(counts.most_common(10))\n",
    "print(sorted(sorted(set(segmented_test_as_subwords)),key=len)[-10:])\n",
    "counts_3=Counter(list(map(len,(segmented_test_as_subwords))))\n",
    "print(counts_3.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdea4f3caa706b65d0027f60298ccf22",
     "grade": true,
     "grade_id": "cell-e7786799f7aeacdc",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is a list of strings\n",
    "assert_equal(type(top_10_by_freq), list)\n",
    "assert_equal(type(top_10_by_freq[0]), str)\n",
    "assert_equal(len(top_10_by_freq), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "565863fc5e9d4a052da9d1c1a025ee58",
     "grade": true,
     "grade_id": "cell-7f4032e3c0f62c07",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is a list of strings\n",
    "assert_equal(type(top_5_by_len), list)\n",
    "assert_equal(type(top_5_by_len[0]), str)\n",
    "assert_equal(len(top_5_by_len), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6adadc8550bfdc91726ba1484d2518d",
     "grade": true,
     "grade_id": "cell-68ac266fbfcf1cef",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks if your answer is a list of integers\n",
    "assert_equal(type(top_5_freqs_of_lens), list)\n",
    "assert_equal(type(top_5_freqs_of_lens[0]), int)\n",
    "assert_equal(len(top_5_freqs_of_lens), 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d145dded4e501b3ff305b063d6c05353",
     "grade": false,
     "grade_id": "cell-f67771f63a0e8969",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.4 <a class=\"anchor\" id=\"subtask_2_4\"></a>\n",
    "### Your thoughts (3 Points)\n",
    "Briefly answer the following questions:\n",
    "\n",
    "1. Describe what will happen if you change the k parameter? How to find a good number for k?\n",
    "\n",
    "2. What are the possible NLP applications that can benefit from the tokenization into subwords? Why?\n",
    "\n",
    "3. How the OOV number for our data can be lowered further without changing anything in the segmentation procedure (k stays the same)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "819985af5d5b7176cd09585c696d6ec9",
     "grade": true,
     "grade_id": "cell-18c675e80eea69ed",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "1. If k is too small, then the subwords will be too short and cannot get long and accurate subwords. If k is too large, then the computation is heavy. To choose a good k, I would say a possible way is to use the percentage of subword types in the test corpus vocabulary that are absent in the training corpus as a measure.\n",
    "\n",
    "2. An example use case is neural machine translation (NMT). It makes the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. As we can see in this exercise, the out-of-vocabulary is dramatically decreased using BPE. So BPE helps the translation of out-of-vocabulary words.\n",
    "\n",
    "3. Enlarge training data set to cover more words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77b73da06a1df442d9b2a15f607f7da5",
     "grade": false,
     "grade_id": "cell-af66e3f2833de7d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything. You might need to run the validation in the terminal because BPE algorithm takes time.\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=718490) section of Mycoures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
